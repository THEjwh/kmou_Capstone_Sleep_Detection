# -*- coding: utf-8 -*-
"""head_pose.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TPosyVN9Fq7o4UPGtmGBbz9F7DhXgelp
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.utils import shuffle

#데이터 읽기
datas = pd.read_csv('./head_Pose_Dataset.csv')

#데이터 섞기
data = shuffle(datas, random_state=32)


target_y = data['is_angled']
target_x = data.drop(['is_angled'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(target_x, target_y, test_size=0.2, random_state=52)


#특성공학
from sklearn.preprocessing import PolynomialFeatures
poly= PolynomialFeatures(include_bias=False)
poly.fit(X_train)
X_train=poly.transform(X_train)
X_test = poly.transform(X_test)


#정규화 단계
from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
ss.fit(X_train)
X_train = ss.transform(X_train)
X_test = ss.transform(X_test)

#경사하강법  
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
sgd_clf = SGDClassifier()
sgd_clf.fit(X_train,y_train)
print("train: ",sgd_clf.score(X_train,y_train))
print("test :",sgd_clf.score(X_test,y_test))
predict = sgd_clf.predict(X_test)
print("정확도 :" ,accuracy_score(y_test,predict))

#엑스트라 트리
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import cross_validate
et = ExtraTreesClassifier(random_state=26)
et.fit(X_train,y_train)
print(et.score(X_train,y_train))
print(et.score(X_test,y_test))

"""결정트리"""

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier(random_state=26)
params={
    "min_samples_split":np.arange(2,20,1),
    "min_samples_leaf":np.arange(1,20,1)
}
grid=GridSearchCV(dt,params)
grid.fit(X_train,y_train)
print(grid.best_params_)
dt_g=grid.best_estimator_
print("train: ",dt_g.score(X_train,y_train))
print("test :",dt_g.score(X_test,y_test))
predict = dt_g.predict(X_test)
print("정확도 :" ,accuracy_score(y_test,predict))

from sklearn.ensemble import RandomForestClassifier
rd =RandomForestClassifier(random_state=26)
params={
    "min_samples_split":np.arange(2,20,1),
    "min_samples_leaf":np.arange(1,20,1)
}
grid=GridSearchCV(rd,params)
grid.fit(X_train,y_train)
print(grid.best_params_)
rd_g=grid.best_estimator_
print("train: ",rd_g.score(X_train,y_train))
print("test :",rd_g.score(X_test,y_test))
predict = rd_g.predict(X_test)
print("정확도 :" ,accuracy_score(y_test,predict))

from sklearn.ensemble import HistGradientBoostingClassifier
hgb=HistGradientBoostingClassifier(random_state=26)
params={
    "min_samples_leaf":np.arange(1,20,1)
}
grid=GridSearchCV(hgb,params)
grid.fit(X_train,y_train)
print(grid.best_params_)
hgb_g=grid.best_estimator_
print("train: ",hgb_g.score(X_train,y_train))
print("test :",hgb_g.score(X_test,y_test))
predict = hgb_g.predict(X_test)
print("정확도 :" ,accuracy_score(y_test,predict))

from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
hgb=HistGradientBoostingClassifier(loss='auto',random_state=26)
params={
    "tol":np.arange(1e-7,1e-6,1e-7),
    "max_depth":np.arange(3,20,1),
    "min_samples_leaf":np.arange(1,20,1)
}
grid=GridSearchCV(hgb,params)
grid.fit(X_train,y_train)
print(grid.best_params_)
hgb_g=grid.best_estimator_
print("train: ",hgb_g.score(X_train,y_train))
print("test :",hgb_g.score(X_test,y_test))
predict = hgb_g.predict(X_test)
print("정확도 :" ,accuracy_score(y_test,predict))